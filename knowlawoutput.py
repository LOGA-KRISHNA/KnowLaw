# -*- coding: utf-8 -*-
"""KnowLawOutput.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y8U16oKMSa3WlKWML57nqTYrKu8jcYy6
"""

# Install necessary libraries
!pip install -U bitsandbytes accelerate transformers gradio

# Import necessary libraries
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import gradio as gr
import re

# Replace with your actual model path
model_name = "logakrishna/fine-tuned-knowlaw-bot"

# Load tokenizer and model with quantization configuration
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Quantization configuration for loading the model in 4-bit
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Enable 4-bit quantization
    bnb_4bit_compute_dtype=torch.bfloat16,  # Set compute dtype to bfloat16
    bnb_4bit_quant_type="nf4",  # Set quantization type to nf4
)

model = AutoModelForCausalLM.from_pretrained(
    model_name, quantization_config=quantization_config
)

# Set device (use GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Function to shorten output to first N sentences
def shorten_answer(text, sentences=2):
    return " ".join(re.split(r'(?<=[.!?]) +', text.strip())[:sentences])

# Main generation function
def generate(input_text):
    # Add explicit instruction for conciseness
    prompt = f"Answer the following legal question in one or two precise sentences only:\n{input_text}"

    # Tokenize and send to device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # Generate concise response
    output = model.generate(
        **inputs,
        max_new_tokens=50,  # Strict limit on output length
        temperature=0.3,    # More focused, less random
        do_sample=True
    )

    # Decode output
    response = tokenizer.decode(output[0], skip_special_tokens=True)

    # Remove prompt text and keep only answer
    if prompt in response:
        response = response.replace(prompt, "").strip()

    # Post-process to 1â€“2 sentences
    response = shorten_answer(response, sentences=2)

    return response

# Gradio interface
iface = gr.Interface(
    fn=generate,
    inputs=gr.Textbox(lines=2, placeholder="Ask a legal question..."),
    outputs="text",
    title="KnowLaw Bot - Precise Legal Answers",
    description="Get concise legal answers based on the Indian Constitution."
)

iface.launch(share=True)  # share=True gives public link